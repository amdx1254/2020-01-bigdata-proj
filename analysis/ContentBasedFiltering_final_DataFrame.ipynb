{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import findspark\n",
    "findspark.init('/home/hadoop/spark-2.2.2-bin-hadoop2.7')\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "\"\"\"\n",
    "\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk1.8.0_241\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark2.2.2\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/aaa/anaconda3/envs/py356/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/aaa/anaconda3/envs/py356/bin/python\"\n",
    "findspark.init()\n",
    "memory = '20g'\n",
    "pyspark_submit_args = '--jars /home/aaa/scala/target/scala-2.11/cosine-similarity_2.11-1.0.jar --master local[24] --driver-memory ' + memory + ' --executor-cores 2 --num-executors 12 --executor-memory 20g --conf spark.ui.port=54353 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '20g')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '20g')\n",
    "sc = SparkContext('local[24]')\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- makerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- achievementRate: integer (nullable = true)\n",
      " |-- totalAmount: integer (nullable = true)\n",
      " |-- totalSupporter: integer (nullable = true)\n",
      " |-- totalLike: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# csv 파일 한도 늘리기 && pandas > spark dataframe 데이터 깨짐\\ncsv.field_size_limit(1000000)\\nfr = open('wadiz_final.csv', 'r', encoding='utf-8')\\nfw = open('wadiz_final2.csv', 'w', encoding='utf-8')\\nrdr = csv.reader(fr)\\nwadiz_list = []\\nwadiz_list.append(['id', 'name', 'category', 'makerName', 'summary', 'achievementRate', \\n                   'totalAmount', 'totalSupporter','totalLike'])\\ncount = 0\\nfor i,line in enumerate(rdr):\\n    if(i == 0):\\n        continue\\n    id = line[4]\\n    name = line[5]\\n    category = line[6]\\n    makerName = line[7]\\n    summary = [line[8]]\\n    achievementRate = line[9]\\n    totalAmount = line[10]\\n    totalSupporter = line[11]\\n    totalLike = line[12]\\n    wadiz_list.append([id, name, category, makerName, summary, achievementRate, totalAmount, \\n                       totalSupporter, totalLike])\\n    \\nwtr = csv.writer(fw)\\nfor row in wadiz_list:\\n    wtr.writerow(row)\\nprint(len(wadiz_list))\\nfr.close()\\nfw.close()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "wadiz_schema = StructType([\n",
    "    StructField(\"idx\", IntegerType()),\n",
    "    StructField(\"Unnamed: 0\", IntegerType()),\n",
    "    StructField(\"Unnamed: 0.1\", IntegerType()),\n",
    "    StructField(\"Unnamed: 0.1.1\", IntegerType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"makerName\", StringType()),\n",
    "    StructField(\"summary\", StringType()),\n",
    "    StructField(\"achievementRate\", IntegerType()),\n",
    "    StructField(\"totalAmount\", IntegerType()),\n",
    "    StructField(\"totalSupporter\", IntegerType()),\n",
    "    StructField(\"totalLike\", IntegerType()),\n",
    "    StructField(\"rewardSatisfaction\", DoubleType()),\n",
    "    StructField(\"makerSatisfaction\", DoubleType()),\n",
    "    StructField(\"detailUrl\", StringType()),\n",
    "    StructField(\"campaigncomments\", StringType()),\n",
    "    StructField(\"comments\", StringType())\n",
    "])\n",
    "spark_df =  spark.read.option(\"multiline\",'true').csv(\"file:///home/aaa/wadiz/wadiz_final.csv\", header=True, inferSchema=True, schema=wadiz_schema)\n",
    "spark_df_droped = spark_df.select(\"id\", \"name\", \"category\", \"makerName\", \"summary\", \"achievementRate\", \"totalAmount\", \"totalSupporter\", \"totalLike\")\n",
    "spark_df_droped.printSchema()\n",
    "\n",
    "spark_df_droped.write.csv(path=\"file:///home/aaa/wadiz/wadiz_final1.csv\", header='true', mode='overwrite')\n",
    "\"\"\"\n",
    "# csv 파일 한도 늘리기 && pandas > spark dataframe 데이터 깨짐\n",
    "csv.field_size_limit(1000000)\n",
    "fr = open('wadiz_final.csv', 'r', encoding='utf-8')\n",
    "fw = open('wadiz_final2.csv', 'w', encoding='utf-8')\n",
    "rdr = csv.reader(fr)\n",
    "wadiz_list = []\n",
    "wadiz_list.append(['id', 'name', 'category', 'makerName', 'summary', 'achievementRate', \n",
    "                   'totalAmount', 'totalSupporter','totalLike'])\n",
    "count = 0\n",
    "for i,line in enumerate(rdr):\n",
    "    if(i == 0):\n",
    "        continue\n",
    "    id = line[4]\n",
    "    name = line[5]\n",
    "    category = line[6]\n",
    "    makerName = line[7]\n",
    "    summary = [line[8]]\n",
    "    achievementRate = line[9]\n",
    "    totalAmount = line[10]\n",
    "    totalSupporter = line[11]\n",
    "    totalLike = line[12]\n",
    "    wadiz_list.append([id, name, category, makerName, summary, achievementRate, totalAmount, \n",
    "                       totalSupporter, totalLike])\n",
    "    \n",
    "wtr = csv.writer(fw)\n",
    "for row in wadiz_list:\n",
    "    wtr.writerow(row)\n",
    "print(len(wadiz_list))\n",
    "fr.close()\n",
    "fw.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- makerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- achievementRate: integer (nullable = true)\n",
      " |-- totalAmount: integer (nullable = true)\n",
      " |-- totalSupporter: integer (nullable = true)\n",
      " |-- totalLike: integer (nullable = true)\n",
      "\n",
      "+-----+----------------------------------------+--------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-----------+--------------+---------+\n",
      "|id   |name                                    |category|makerName  |summary                                                                                                                                                                                                |achievementRate|totalAmount|totalSupporter|totalLike|\n",
      "+-----+----------------------------------------+--------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-----------+--------------+---------+\n",
      "|65544|[1.8억/7차] 묵은 피지와 각질도 끝장보는 클렌저 <세수한번 굿모링>|뷰티      |심플리웍스      |진짜가 돌아왔다! 더 강력해진 <세수한번 굿모링 클렌저>가 블랙헤드, 각질은 물론 피부결 개선까지 책임질게요. 올 여름 피부, 세수한번만 믿고 따라오세요!                                                                                                                 |7906           |39532400   |580           |426      |\n",
      "|9    |서울 강남 한복판에 카페를 함께 만든다?!                 |소셜·캠페인  |쉘위 Shall We|Shall We make 'Shall We?'\n",
      "여러분의 이름을 새겨 함께 만드는\n",
      "카페 'Shall We?' 2호점!\n",
      "\n",
      "'Shall We?' 에서 일어나는 착한 활동들을 응원해주세요!                                                                                                  |103            |4135000    |147           |14       |\n",
      "|11   |일본에 빼앗긴 한국의 얼 우리 도예를 지켜주세요!             |디자인소품   |심천요        |여러분은 우리의 도자기에 대해 얼마나 알고 계십니까?                                                                                                                                                                          |105            |2105000    |58            |13       |\n",
      "|12   |자연 농업, 내 아이의 건강을 지켜주세요                  |푸드      |자연농업연구소    |농약쌀이 친환경 쌀로 둔갑되어 학교 급식으로 납품!\n",
      "혹시 귀하의 자녀가 농약쌀을 먹고 있지는 않습니까?\n",
      "\n",
      "요즘 임신한 엄마들 양수가 초콜렛 색입니다. 엄마가 제대로 된 건강한 음식을 못먹어서 그 아이에게까지 영향을 끼치는 것입니다. 밭이 건강해야 건강한 자식이 태어납니다. - 자연농업 조한규 원장                                 |102            |2040000    |51            |12       |\n",
      "|13   |저, 일기예보 나들 이야기 한번 들어보실래요?               |공연·컬쳐   |나들         |저는 지금 골목상권을 살리기 위한 골목콘서트를 진행하고 있습니다. 솔직히 혼자서 550만 골목상권을 살리는건 불가능하죠..\n",
      "\n",
      "다만, 저는 노래를 통해 골목상권을 살리기 위해 계속 노력할 것이고, 이번에 여러분을 위해 제가 특별히 준비한 콘서트에 와 주셔서 마음껏 즐기시면 됩니다. 그럼 여러분은 이미 골목상권을 살리고 있는 대한민국 1% 가 되는 것입니다.|21             |1060000    |48            |13       |\n",
      "+-----+----------------------------------------+--------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-----------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df =  spark.read.option(\"multiline\",'true').csv(\"file:///home/aaa/wadiz/wadiz_final1.csv\", header=True, inferSchema=True)\n",
    "spark_df.na.drop(subset=[\"category\"])\n",
    "spark_df.na.drop(subset=[\"makerName\"])\n",
    "spark_df.na.drop(subset=[\"summary\"])\n",
    "spark_df.printSchema()\n",
    "spark_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- makerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- achievementRate: integer (nullable = true)\n",
      " |-- totalAmount: integer (nullable = true)\n",
      " |-- totalSupporter: integer (nullable = true)\n",
      " |-- totalLike: integer (nullable = true)\n",
      " |-- soop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, lit\n",
    "\n",
    "def summary_type(summary):\n",
    "    if(summary is None):\n",
    "        return '0'\n",
    "    else:\n",
    "        return summary.replace('\\n', '')\n",
    "\n",
    "def makerName_type(makerName):\n",
    "    if(makerName is None):\n",
    "        return '0'\n",
    "    else:\n",
    "        return makerName\n",
    "    \n",
    "def category_type(category):\n",
    "    if(category is None):\n",
    "        return '0'\n",
    "    else:\n",
    "        return category    \n",
    "\n",
    "summary_udf = udf(summary_type, StringType())  \n",
    "makerName_udf = udf(makerName_type, StringType())  \n",
    "category_udf = udf(category_type, StringType())  \n",
    "soop = udf(lambda category, makerName, summary: (category)*1 + (makerName + ' ')*2 + summary)\n",
    "spark_df = spark_df.withColumn(\"id\", spark_df['id'].cast('integer'))\n",
    "spark_df = spark_df.withColumn(\"summary\", summary_udf(spark_df['summary']))\n",
    "spark_df = spark_df.withColumn(\"makerName\", makerName_udf(spark_df['makerName']))\n",
    "spark_df = spark_df.withColumn(\"category\", category_udf(spark_df['category']))\n",
    "spark_df = spark_df.withColumn(\"achievementRate\", spark_df['achievementRate'].cast('integer'))\n",
    "spark_df = spark_df.withColumn(\"totalAmount\", spark_df['totalAmount'].cast('integer'))\n",
    "spark_df = spark_df.withColumn(\"totalSupporter\", spark_df['totalSupporter'].cast('integer'))\n",
    "spark_df = spark_df.withColumn(\"totalLike\", spark_df['totalLike'].cast('integer'))\n",
    "spark_df = spark_df.withColumn(\"soop\", soop(spark_df['category'], spark_df['makerName'], spark_df['summary']).cast('string'))\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- makerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- achievementRate: integer (nullable = true)\n",
      " |-- totalAmount: integer (nullable = true)\n",
      " |-- totalSupporter: integer (nullable = true)\n",
      " |-- totalLike: integer (nullable = true)\n",
      " |-- soop: string (nullable = true)\n",
      " |-- rangeAmount: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dividePrice(totalAmount, totalSupporter):\n",
    "    if(totalSupporter is None or totalAmount is None):\n",
    "        return 0\n",
    "    if (totalSupporter == 0 or totalAmount == 0):\n",
    "        return 0\n",
    "    \n",
    "    price = int(totalAmount / totalSupporter)\n",
    "    if 0 <= price and price <30000:\n",
    "        return 0\n",
    "    elif 30000<= price and price <50000:\n",
    "        return 1\n",
    "    elif 50000<= price and price <70000:\n",
    "        return 2\n",
    "    elif 70000<= price and price <100000:\n",
    "        return 3\n",
    "    elif 100000<= price and price <200000:\n",
    "        return 4\n",
    "    elif 200000<= price and price< 300000:\n",
    "        return 5\n",
    "    elif 300000<= price and price <400000:\n",
    "        return 6\n",
    "    elif 400000<= price and price <500000:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8\n",
    "    \n",
    "def divideAmount(totalAmount, totalSupporter):\n",
    "    if(totalSupporter is None or totalAmount is None):\n",
    "        return 0\n",
    "    if (totalSupporter == 0 or totalAmount == 0):\n",
    "        return 0\n",
    "    price = int(totalAmount / totalSupporter)\n",
    "    return price\n",
    "    \n",
    "price_udf = udf(dividePrice, IntegerType())\n",
    "amount_udf = udf(divideAmount, IntegerType())\n",
    "spark_df = spark_df.withColumn('rangeAmount', price_udf(spark_df['totalAmount'], spark_df['totalSupporter']))\n",
    "spark_df = spark_df.withColumn('amount', amount_udf(spark_df['totalAmount'], spark_df['totalSupporter']))\n",
    "spark_df = spark_df.withColumn('soop', spark_df['soop'].cast('string'))\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing and Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+--------------+--------------------+---------------+-----------+--------------+---------+--------------------+-----------+------+\n",
      "|   id|                name|category|     makerName|             summary|achievementRate|totalAmount|totalSupporter|totalLike|                soop|rangeAmount|amount|\n",
      "+-----+--------------------+--------+--------------+--------------------+---------------+-----------+--------------+---------+--------------------+-----------+------+\n",
      "|65544|[1.8억/7차] 묵은 피지와 ...|      뷰티|         심플리웍스|진짜가 돌아왔다! 더 강력해진 ...|           7906|   39532400|           580|      426|뷰티심플리웍스 심플리웍스 진짜가...|          2| 68159|\n",
      "|    9|서울 강남 한복판에 카페를 함께...|  소셜·캠페인|   쉘위 Shall We|Shall We make 'Sh...|            103|    4135000|           147|       14|소셜·캠페인쉘위 Shall We...|          0| 28129|\n",
      "|   11|일본에 빼앗긴 한국의 얼 우리 ...|   디자인소품|           심천요|여러분은 우리의 도자기에 대해 ...|            105|    2105000|            58|       13|디자인소품심천요 심천요 여러분은...|          1| 36293|\n",
      "|   12|자연 농업, 내 아이의 건강을 ...|      푸드|       자연농업연구소|농약쌀이 친환경 쌀로 둔갑되어 ...|            102|    2040000|            51|       12|푸드자연농업연구소 자연농업연구소...|          1| 40000|\n",
      "|   13|저, 일기예보 나들 이야기 한번...|   공연·컬쳐|            나들|저는 지금 골목상권을 살리기 위...|             21|    1060000|            48|       13|공연·컬쳐나들 나들 저는 지금 ...|          0| 22083|\n",
      "|65566|[단백질쉐이크 정착지] 이거 흑...|      푸드|           프롬잇|카페 인기 메뉴가 단백질쉐이크라...|          11423|  114230300|          1673|      669|푸드프롬잇 프롬잇 카페 인기 메...|          2| 68278|\n",
      "|   31|  직장인 페스티벌 with 조조에코|   공연·컬쳐|           김경래|직장인 여러분! 함께 축제를 열...|            111|    2235000|           134|        5|공연·컬쳐김경래 김경래 직장인 ...|          0| 16679|\n",
      "|65568|[클라] 초경량, 가성비, 내구...|   패션·잡화|           엠엔씨|왜? 노트북 가방이 무겁고 비싸...|            166|    1661700|            63|       50|패션·잡화엠엔씨 엠엔씨 왜? 노...|          0| 26376|\n",
      "|65570|긴급)'마포FM의 오랜 꿈' 코...|  소셜·캠페인|      마포공동체라디오|다양한 공연, 파티, 강연, 회...|            114|    1145000|            40|       13|소셜·캠페인마포공동체라디오 마포...|          0| 28625|\n",
      "|   40|함께 만드는 새로운 크리스천 브...|   디자인소품|           신경재|이미 크리스마스는 예수님 없이도...|              3|      95000|            29|        0|디자인소품신경재 신경재 이미 크...|          0|  3275|\n",
      "|   43|진심 어린 음악의 싱어송라이터 ...|   공연·컬쳐|          동네빵집|동네빵집은 ‘여러분의 이야기’입...|            102|    3060000|            79|       11|공연·컬쳐동네빵집 동네빵집 동네...|          1| 38734|\n",
      "|   49|국내 최초의 크라우드펀딩 서적 ...|      출판|     크라우드산업연구소|크라우드펀딩...여기저기서 많이...|            101|    3050000|           101|        6|출판크라우드산업연구소 크라우드산...|          1| 30198|\n",
      "|65586|어디에나 설치 가능한  공간활용...|   디자인소품|         에스퍼니처|공간활용의 끝판왕! 아리부 흡착...|            194|     972000|            52|       79|디자인소품에스퍼니처 에스퍼니처 ...|          0| 18692|\n",
      "|   51|당신이 생각하는 아프리카는 어떤...|   공연·컬쳐|Soul of Africa|당신이 생각하는 <아프리카>는 ...|            118|    1540000|            52|        0|공연·컬쳐Soul of Afri...|          0| 29615|\n",
      "|   55|[1mmACT]말라위 농부들에게...|  소셜·캠페인|        1mmACT|Please be our Hid...|            113|    1470000|            61|        0|소셜·캠페인1mmACT 1mmA...|          0| 24098|\n",
      "|   57|단언컨대, 산들농장 달걀은 가장...|      푸드|           이재국|산들농장에서는 닭들을 자연 속에...|            139|    1397000|            55|        1|푸드이재국 이재국 산들농장에서는...|          0| 25400|\n",
      "|   59|우간다 청년들이 능력을 발휘할 ...|  소셜·캠페인|         TELLA|영어를 배우는 가장 따뜻한 방법...|            101|    1313000|            53|        1|소셜·캠페인TELLA TELLA...|          0| 24773|\n",
      "|65598|[코로나도돕자!] 레모니모니해도...|   디자인소품|           이주영|나도 대한민국을 도울 수 있다?...|             46|     394000|            51|       28|디자인소품이주영 이주영 나도 대...|          0|  7725|\n",
      "|   63|바이맘 실내 보온텐트로 모두가 ...|  소셜·캠페인|        (주)바이맘|세상을 따뜻하게 만들고 싶은 청...|            311|   15556000|           176|        1|소셜·캠페인(주)바이맘 (주)바...|          3| 88386|\n",
      "|   66|38만 대학생의 가을운동회, 대...|   공연·컬쳐|         대정대학교|\"    대학간 서열 매기기, ...|           null|       null|          null|     null|공연·컬쳐대정대학교 대정대학교 ...|          0|     0|\n",
      "+-----+--------------------+--------+--------------+--------------------+---------------+-----------+--------------+---------+--------------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "spark_df.registerTempTable('wadiz')\n",
    "res= sqlContext.sql('SELECT id, name, category, makerName, summary, achievementRate ,soop, rangeAmount ,amount from wadiz')\n",
    "res.show()\n",
    "\"\"\"\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,HashingTF, Word2Vec\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='soop', outputCol='keywords')\n",
    "wordData = tokenizer.transform(spark_df)\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol='keywords', outputCol='word_vec', seed=123)\n",
    "word2VecData = word2Vec.fit(wordData)\n",
    "word2VecData = word2VecData.transform(wordData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rangeAmount 비교값 추가\n",
    "#all_wadiz_vecs = word2VecData.select('id','word_vec','rangeAmount').rdd.map(lambda x: (x[0], x[1], x[2])).collect()\n",
    "all_wadiz_vecs = word2VecData.select('id','word_vec','rangeAmount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'word_vec'>\n",
      "Column<b'word_vec[0]'>\n",
      "Column<b'word_vec[1]'>\n",
      "Column<b'word_vec[2]'>\n"
     ]
    }
   ],
   "source": [
    "print(all_wadiz_vecs[1])\n",
    "print(all_wadiz_vecs[1][0])\n",
    "print(all_wadiz_vecs[1][1])\n",
    "print(all_wadiz_vecs[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSim(vec1, vec2): \n",
    "    return vec1.dot(vec2)/(vec1.norm(2)*vec2.norm(2))\n",
    "    #return np.dot(vec1, vec2) / np.sqrt(np.dot(vec1, vec1)) / np.sqrt(np.dot(vec2, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql.column import Column, _to_java_column, _to_seq \n",
    "\n",
    "def cosinesimilarity_udf(a, b): \n",
    "    cosinesimilarityUDF = spark._jvm.cosinesimilarityUDFs.cosinesimilarityUDF() \n",
    "    return Column(cosinesimilarityUDF.apply(_to_seq(spark.sparkContext, [a, b], _to_java_column)))\n",
    "\n",
    "# input: 상품 id 리스트 > output: 추천 상품 (input_id(입력한 상품 id), id(입력한 상품과 코사인 유사도가 제일 높은 상품 id), score(코사인 유사도 계산)))\n",
    "def getSimilarProduct(w_ids, sim_product_limit=10):\n",
    "    \n",
    "    all_wadiz_vecs_wids = all_wadiz_vecs.where(all_wadiz_vecs.id.isin(wids))\n",
    "    all_wadiz_no_wids = all_wadiz_vecs.filter(~all_wadiz_vecs.id.isin(wids))\n",
    "    all_wadiz_vecs_renamed = all_wadiz_no_wids.select(F.col('id').alias('id2'), F.col('word_vec').alias('word_vec2'), F.col('rangeAmount').alias('rangeAmount2'))\n",
    "    all_wadiz_vecs_joined = all_wadiz_vecs_wids.join(all_wadiz_vecs_renamed,[all_wadiz_vecs_renamed.rangeAmount2 > all_wadiz_vecs_wids.rangeAmount - 2,all_wadiz_vecs_renamed.rangeAmount2  < all_wadiz_vecs_wids.rangeAmount + 2] ,'inner')\n",
    "    all_wadiz_vecs_joined = all_wadiz_vecs_joined.withColumn('score', cosinesimilarity_udf(all_wadiz_vecs_joined.word_vec,all_wadiz_vecs_joined.word_vec2)).select(F.col('id2').alias('id'), F.col('score'), F.col('id').alias('input_id'))\n",
    "    all_wadiz_vecs_joined = all_wadiz_vecs_joined.na.fill(0.0, 'score').orderBy( \"score\", ascending=False).limit(8)\n",
    "            \n",
    "    return all_wadiz_vecs_joined\n",
    "    \n",
    "    \"\"\"\n",
    "    schema = StructType([  \n",
    "                        StructField(\"id\", StringType(), True)\n",
    "                        ,StructField(\"score\", IntegerType(), True)\n",
    "                        ,StructField(\"input_id\", StringType(), True)\n",
    "                        ])\n",
    "    similar_products_df = spark.createDataFrame([], schema)\n",
    "    print(w_ids, type(w_ids))\n",
    "    for w_id in w_ids:\n",
    "        w_id = int(w_id)\n",
    "        input_vec = []\n",
    "        for r in all_wadiz_vecs:\n",
    "            if r[0] == w_id:\n",
    "                input_vec = [(r[1])][0]\n",
    "                input_rangeAmount = r[2]\n",
    "            else:\n",
    "                continue              \n",
    "            \n",
    "            similar_product_rdd = sc.parallelize((i[0], float(CosineSim(input_vec, i[1]))) for i in all_wadiz_vecs if (input_rangeAmount - 2 < i[2] and input_rangeAmount + 2 > i[2]))  \n",
    "            similar_product_df = spark.createDataFrame(similar_product_rdd)\\\n",
    "                            .withColumnRenamed('_1', 'id')\\\n",
    "                            .withColumnRenamed('_2', 'score')\\\n",
    "                            .orderBy('score', ascending = False)\n",
    "            similar_product_df = similar_product_df.filter(col(\"id\") != w_id).limit(sim_product_limit)\n",
    "            similar_product_df = similar_product_df.filter(col(\"score\") <= 1)\n",
    "            similar_product_df = similar_product_df.withColumn('input_id', lit(w_id))\n",
    "            similar_products_df = similar_products_df \\\n",
    "                                        .union(similar_product_df)\n",
    "    return similar_products_df\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 추천 상품 (input_id, id, score) > output: inner join (input_id, id, score, name, category, makerName, summary)\n",
    "def getProductDetails(in_product):\n",
    "    a = in_product.alias(\"a\")\n",
    "    b = spark_df.alias(\"b\")\n",
    "    \n",
    "    return a.join(b, col(\"a.id\") == col(\"b.id\"), 'inner') \\\n",
    "             .select([col('a.'+xx) for xx in a.columns] + [col('b.name'),col('b.category'),\n",
    "                                                           col('b.makerName'),col('b.summary'),\n",
    "                                                          col('b.rangeAmount'), col('b.amount')])                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 ms, sys: 6.11 ms, total: 30.2 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "\n",
    "# 상품 id 리스트\n",
    "wids = [54968, 53536, 42496, 30763, 34841]\n",
    "\n",
    "# 상품 추천 및 추천 상품 디테일 \n",
    "sims = getProductDetails(getSimilarProduct(wids))\n",
    "sims.select('id','name','summary','category','makerName', 'amount','score').orderBy('score').limit(8).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User based Recomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df =  spark.read.csv(\"file:///home/aaa/wadiz/users_merge_result2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  user_id|funding_id|\n",
      "+---------+----------+\n",
      "| 96394201|     65544|\n",
      "| 96394201|     29175|\n",
      "| 96394201|     43153|\n",
      "|298659501|     65544|\n",
      "|298659501|     68610|\n",
      "|298659501|     59790|\n",
      "|107362301|     65544|\n",
      "|107362301|     68436|\n",
      "|107362301|     30971|\n",
      "|107362301|     63273|\n",
      "|107362301|     54432|\n",
      "|107362301|     62812|\n",
      "|107362301|     59605|\n",
      "|107362301|     57895|\n",
      "|107362301|     57078|\n",
      "|107362301|     51684|\n",
      "|107362301|     54086|\n",
      "|107362301|     51419|\n",
      "|107362301|     48746|\n",
      "|107362301|     46396|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- funding_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.select('user_id' ,'funding_id').show()\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "user_rdd = user_df.select('*').groupby('user_id').agg(F.collect_list('funding_id').alias('funding_ids')).rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29601 [9, 9, 13, 275, 642, 642, 689, 733, 750, 750, 9265, 44833]\n",
      "47501 [31]\n",
      "57201 [31]\n",
      "206501 [55]\n",
      "227501 [66]\n",
      "291501 [69, 69]\n",
      "300601 [63, 58743]\n",
      "312801 [49]\n",
      "317201 [49, 9271, 16853]\n",
      "324601 [1094, 42262]\n",
      "353501 [87]\n",
      "400301 [87]\n",
      "441201 [142, 65614, 52236, 58339, 58274]\n",
      "450801 [142]\n",
      "460601 [136]\n",
      "529301 [228]\n",
      "570901 [177, 177, 62607, 62607]\n",
      "664901 [195]\n",
      "688301 [183]\n",
      "701901 [239, 239, 457]\n",
      "704301 [55978]\n",
      "849401 [253]\n",
      "867601 [253]\n",
      "883701 [1553, 1553]\n",
      "888901 [285, 890]\n",
      "909801 [185]\n",
      "928901 [317]\n",
      "953701 [365]\n",
      "955001 [340, 554, 755]\n",
      "982701 [405]\n",
      "1034201 [366]\n",
      "1062801 [366]\n",
      "1068901 [416]\n",
      "1078201 [430]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3504dc382711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfunding_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunding_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetProductDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetSimilarProduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunding_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ed7bf2976a6f>\u001b[0m in \u001b[0;36mgetSimilarProduct\u001b[0;34m(w_ids, sim_product_limit)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#all_wadiz_vecs_joined = all_wadiz_vecs_joined.withColumn('score', dot_udf(all_wadiz_vecs_joined.norm,all_wadiz_vecs_joined.norm2)).select(F.col('id2').alias('id'), F.col('score'), F.col('id').alias('input_id'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mall_wadiz_vecs_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_wadiz_vecs_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosinesimilarity_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_wadiz_vecs_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_wadiz_vecs_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mall_wadiz_vecs_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_wadiz_vecs_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_wadiz_vecs_joined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(self, value, subset)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[0mfill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, subset)\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subset should be a list or tuple of column names\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark2.2.2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aaa/anaconda3/envs/py356/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# user_df = user_rdd.toDf()\n",
    "for i, r in enumerate(user_rdd):\n",
    "    u_id = r[0]\n",
    "    funding_ids = list(r[1])\n",
    "    print(u_id, funding_ids)\n",
    "    sims = getProductDetails(getSimilarProduct(list(set(funding_ids))))\n",
    "    sims = sims.withColumn('user_id', lit(u_id))\n",
    "    sims = sims.select('user_id','id','name','category', 'amount','score').orderBy('score').limit(10)\n",
    "    if(i == 0):\n",
    "        allSims = sims\n",
    "    else:\n",
    "        allSims = allSims.union(sims)\n",
    "        \n",
    "    if(i == len(user_rdd)):\n",
    "        allSims.to_json('./cbf_recommend.json', orient='records', lines=True)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
